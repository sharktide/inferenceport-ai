<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Help â€” Ollama Models</title>
    <link rel="stylesheet" href="help.css" />
    <script type="module" src="../scripts/staticload/theme.js"></script>
  </head>
  <body>
    <nav>
      <div class="logo">âš¡InferncePort AI | Help</div>
      <ul class="nav-links">
        <li><a href="index.html">Back</a></li>
        <li><a href="javascript:window.close()">Close</a></li>
      </ul>
    </nav>

    <main style="padding: 40px">
      <h2>ðŸ¤– Ollama Models â€” What they are</h2>
      <div class="highlight">
        <p>
          Ollama models are self-contained AI model packages you can
          download and run on your computer. Running them locally keeps your
          data private and can be faster than cloud APIs.
        </p>

        <h3>How to pull a model</h3>
        <ul>
          <li>Open <strong>Marketplace â†’ Ollama</strong>.</li>
          <li>Choose a model and click <strong>Pull</strong>. Progress will
          display in the UI.</li>
          <li>When complete, the model appears in the models list.</li>
        </ul>

        <h3>Managing models</h3>
        <ul>
          <li><strong>Run:</strong> Start the model for chat or inference.</li>
          <li><strong>Delete:</strong> Removes model files to free disk space.</li>
          <li><strong>Warnings:</strong> Large models need more disk and RAM.
          Check system requirements before pulling.</li>
        </ul>

        <h3>Troubleshooting</h3>
        <ul>
          <li>If a pull fails, check your internet connection and disk space.</li>
          <li>If the model won't run, open Developer Tools from the App menu
          to view logs.</li>
        </ul>
      </div>
    </main>
  </body>
</html>
