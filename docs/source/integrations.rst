Integrations
============

InferencePort AI can integrate with various tools and platforms to enhance your workflow. Below are some of the key integrations available:

1. **Ollama**: For local model inference, InferencePort AI can connect to Ollama, allowing you to run models directly on your machine without needing a cloud service.

2. **Hugging Face Spaces**: You can launch Hugging Face Spaces directly from the app, making it easy to access and interact with models hosted on Hugging Face.

3. **Microsoft Store**: For Windows users, InferencePort AI is available on the Microsoft Store, providing a convenient installation option with automatic updates.

4. **Cross-platform support**: InferencePort AI is designed to work seamlessly across Windows, macOS, and Linux, ensuring that you can use it regardless of your operating system.

5. **Optional sign-in features**: While you can use the app locally without signing in, optional sign-in allows for account features and sync across devices, enhancing your experience if you use multiple machines.

6. **HuggingFace GGUF Support** Launch GGUF models from HuggingFace directly in the app, making it easier to access a wide range of models without manual setup.

7. **Remote host support**: For power users, InferencePort AI can connect to remote hosts, allowing you to run models on more powerful machines while still managing them from your local app.

8. **Developer tools**: For developers, InferencePort AI provides tools and documentation for building from source, allowing for customization and contribution to the project.

9. **Model marketplace**: The built-in marketplace allows you to browse, download, and manage models directly from the app, streamlining the process of finding and using AI models.

10. **Chat interface**: The app provides a user-friendly chat interface for interacting with models, making it easy to test and use models without needing to write code.

11. **Conversation management**: Save and reopen conversations later, allowing you to keep track of your interactions with models over time.

12. **Cross-platform file formats**: InferencePort AI supports various file formats for models and conversations, ensuring compatibility across different operating systems and workflows.

13. **Integration with local and cloud models**: Whether you prefer to run models locally or access them from the cloud, InferencePort AI provides the flexibility to work with both types of models seamlessly.

14. **Custom model imports**: For advanced users, InferencePort AI allows you to import custom models, giving you the freedom to use models that may not be available in the marketplace.

15. **Automatic updates**: When installed from the Microsoft Store, InferencePort AI can receive automatic updates, ensuring you always have the latest features and improvements without needing to manually check for updates.

16. **Community support**: InferencePort AI has an active community of users and developers, providing support, sharing models, and contributing to the ongoing development of the app.

17. **Lightning**: InferencePort AI has just introduced two new models called Lightning Text v2 and Lightning Image Turbo, which provide excellent speed and realistic images, all with zero data collection. (See the `privacy policy <https://inference.js.org/security.html#privacy>`_ for details on data collection and usage).