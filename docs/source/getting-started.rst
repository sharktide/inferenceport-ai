Getting Started
===============

This page is for regular users installing InferencePort AI from
``https://inference.js.org``.

Before you install
------------------

* Confirm your operating system (Windows, macOS, or Linux).
* Decide whether you need standard install or accelerator-specific build.
* If you want local model inference, install Ollama if it is not already
  available on your machine.

Use :doc:`download-guide` if you are not sure which file to download.

Install steps
-------------

1. Open ``https://inference.js.org``.
2. Download the installer/package that matches your OS.
3. Run the installer (or open the package) and complete setup.
4. Launch InferencePort AI from your Applications/Start menu.

First launch checklist
----------------------

When the app opens:

1. Go to Marketplace and confirm you can browse models.
2. Download a model (if you do not have one installed yet).
3. Open Chat and send a test prompt.
4. Open Settings and review optional sign-in/sync features.

Optional sign-in
----------------

You can use the app locally without signing in. Sign-in is optional and mainly
used for account features and sync.

Need source build instructions?
-------------------------------

If you want to build the app from source code instead of downloading from the
website, see :doc:`developer/build-release`.
