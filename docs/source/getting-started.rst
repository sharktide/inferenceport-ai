Getting Started
===============

This guide is for first-time users installing InferencePort AI from
``https://inference.js.org``.

Before You Install
------------------

* Confirm your OS: Windows, macOS, or Linux.
* If you are unsure which download to choose, use :doc:`download-guide`.
* If you plan to run local models, make sure Ollama is available.

Install InferencePort AI
------------------------

1. Open ``https://inference.js.org/install.html``.
2. Download the installer/package that matches your OS.
3. Run the installer (or open the package) and complete setup.
4. Open InferencePort AI from Start Menu/Applications.

First Launch (Simple Path)
--------------------------

Use this quick path to confirm everything works:

1. Open ``Marketplace``.
2. Open ``Local Chat`` (Ollama models) and download one model.
3. Go to ``Chat``, select host and model, then send a test prompt.
4. Confirm you receive a streaming response.

Optional Setup
--------------

You can skip this section and come back later.

* Sign in (optional): required only for account features and sync.
* Enable sync (optional): off by default.
* Add remote hosts (optional): if you use model servers on another machine.
* Import models (optional): ``.gguf``/``Modelfile`` from Marketplace.

What To Learn Next
------------------

* :doc:`user-guide` for all app features
* :doc:`troubleshooting` if something fails

Need source build instructions?
-------------------------------

If you want to build the app from source code instead of downloading from the
website, see :doc:`developer/build-release`.
